{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b43c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import spacy\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import Document\n",
    "from dateutil import parser as dateparser\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialize spaCy once\n",
    "NLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2668c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should expand these lists for production!\n",
    "SKILLS_LIST = [\n",
    "    'Python', 'SQL', 'PySpark', 'Shell', 'R', 'NLTK', 'TensorFlow', 'Pandas', 'Scikit-Learn', 'NumPy',\n",
    "    'TFDV', 'PyTorch', 'Airflow', 'ML Flow', 'statsmodels', 'Dask', 'pydantic', 'DASH', 'AWS',\n",
    "    'Azure', 'GCP', 'Snowflake', 'Apache Spark', 'Hadoop', 'dbt', 'Talend', 'Informatica', 'SSIS',\n",
    "    'TIDAL', 'Oracle', 'SQL Server', 'PostgreSQL', 'MySQL', 'Teradata', 'MongoDB', 'Cosmos DB',\n",
    "    'NoSQL', 'Apache Kafka', 'Apache Flink', 'Docker', 'Kubernetes', 'Terraform', 'GitHub Actions',\n",
    "    'CI/CD', 'Power BI', 'Tableau', 'EDA', 'Statistical Modeling', 'Trend Analysis', 'matplotlib',\n",
    "    'seaborn', 'Plotly', 'Agile-Scrum', 'Kanban', 'Data Modelling', 'Data Warehousing', 'GDPR/HIPAA compliance',\n",
    "    'OpenAI embeddings', 'ChromaDB', 'RAG pipelines', 'Supervised & Unsupervised Learning', 'Feature Engineering', 'Model Evaluation metrics'\n",
    "]\n",
    "DEGREE_KEYWORDS = [\n",
    "    'bachelor', 'master', 'doctor', 'phd', 'msc', 'bachelors', 'masters', 'engineering', 'm.tech', 'b.tech'\n",
    "]\n",
    "CERT_KEYWORDS = ['certification', 'certificate', 'certified', 'certifications', 'licenses']\n",
    "SECTION_HEADERS = {\n",
    "    'education': ['education', 'academic background', 'academics'],\n",
    "    'experience': ['professional experience', 'work experience', 'employment', 'experience'],\n",
    "    'skills': ['skills', 'technical skills', 'key skills'],\n",
    "    'certifications': ['certifications', 'certificates', 'licenses'],\n",
    "    'projects': ['projects', 'key projects', 'personal projects']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eb9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def docx_to_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_file(path):\n",
    "    if path.endswith('.pdf'):\n",
    "        return pdf_to_text(path)\n",
    "    elif path.endswith('.docx'):\n",
    "        return docx_to_text(path)\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5a87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    lines = text.split('\\n')\n",
    "    section_map = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    def header_key(line):\n",
    "        line_clean = line.strip().lower()\n",
    "        for key, variants in SECTION_HEADERS.items():\n",
    "            if any(line_clean.startswith(h) for h in variants):\n",
    "                return key\n",
    "        return None\n",
    "\n",
    "    for line in lines:\n",
    "        section = header_key(line)\n",
    "        if section:\n",
    "            if current_section and buffer:\n",
    "                section_map[current_section] = '\\n'.join(buffer).strip()\n",
    "                buffer = []\n",
    "            current_section = section\n",
    "        elif current_section:\n",
    "            buffer.append(line)\n",
    "    # Capture last section\n",
    "    if current_section and buffer:\n",
    "        section_map[current_section] = '\\n'.join(buffer).strip()\n",
    "    return section_map\n",
    "\n",
    "def extract_skills(skills_text):\n",
    "    skills_found = set()\n",
    "    text_lower = skills_text.lower()\n",
    "    '''\n",
    "    for skill in SKILLS_LIST:\n",
    "        if re.search(r'\\b' + re.escape(skill.lower()) + r'\\b', text_lower):\n",
    "            skills_found.add(skill)\n",
    "    '''\n",
    "    # Add anything in a comma/list line in skills section\n",
    "    for line in skills_text.split('\\n'):\n",
    "        if ',' in line:\n",
    "            for word in line.split(','):\n",
    "                word_clean = word.strip()\n",
    "                if word_clean and word_clean not in skills_found:\n",
    "                    skills_found.add(word_clean)\n",
    "    return list(skills_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063e3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=extract_text_from_file(r\"C:\\Users\\hrith\\Projects\\Screening_Agent\\data\\resumes\\Hrithik_Resume.pdf\")\n",
    "sections = extract_sections(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46935837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': '• \\n• \\n• \\n• \\n• \\n\\nProgramming & Scripting: Python, SQL,\\u202fPySpark, Shell, R. \\n\\nPython Libraries: NLTK, TensorFlow, Pandas, Scikit-Learn, NumPy, TFDV, PyTorch, Airflow, ML Flow, statsmodels, Dask, pydantic, DASH \\n\\nCloud & Warehousing: AWS\\u202f(S3, Glue, Redshift, Lambda,\\u202fKinesis, EMR), Azure\\u202f(Synapse, ADLS,\\u202fDatabricks, Data\\u202fFactory), GCP\\u202f(BigQuery, VertexAI), Snowflake. \\n\\nData Processing & Orchestration: Apache\\u202fSpark, Hadoop, dbt, Airflow, Talend, Informatica, SSIS, TIDAL. \\n\\nDatabases: Oracle, SQL\\u202fServer, PostgreSQL, MySQL, Teradata, MongoDB, Cosmos\\u202fDB, NoSQL. \\n\\n• \\n\\nStreaming & Real‑Time Analytics: Apache\\u202fKafka, Apache\\u202fFlink, AWS\\u202fKinesis. \\n\\nContainerization & DevOps: Docker, Kubernetes, Terraform, GitHub\\u202fActions, CI/CD. \\n\\nVisualization & BI: Power BI, Tableau, EDA, Statistical Modeling, Trend Analysis, matplotlib, seaborn, Plotly \\n\\n• \\n• \\n• \\n•  Methodologies & Governance: Agile‑Scrum, Kanban, Data\\u202fModelling, Data\\u202fWarehousing, GDPR/HIPAA compliance. \\n\\nGenAI & Machine Learning: OpenAI embeddings, ChromaDB, RAG pipelines, Supervised & Unsupervised Learning, Feature Engineering, Model Evaluation metrics.',\n",
       " 'experience': 'Senior Data Scientist, Pfizer, USA \\n• \\n\\nAug 2024 – Present \\nDeveloped a RAG pipeline ingesting PDFs, chunking text, embedding with OpenAI text-embedding-3-small, and storing vectors in ChromaDB for <100ms semantic search. \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\nIntegrated GenAI-powered pipelines to automate feature extraction, data augmentation, and metadata management for AI model deployment. \\nAutomated ETL processes using Apache Airflow and Talend, reducing manual intervention by 40% and improving data flow reliability. \\nProductionised a fully-automated sepsis-prediction pipeline on GCP (BigQuery, Airflow, Kubernetes); orchestrated MLflow model training and Vertex\\u202fAI hyper-tuning, \\nachieving 93% accuracy\\u202f/\\u202f0.95\\u202fF1 and continuous retraining via Cloud\\u202fComposer. \\nTrained a transfer-learning Inception-V3 model on 33\\u202fk dermoscopic images; leveraged GAN-based augmentation to correct class imbalance (1:2 ratio) and delivered 98.8% \\naccuracy with 0.95 precision–recall, supporting faster clinical diagnosis. \\nBuilt and optimized data pipelines in Python, SQL, and PySpark with Apache Spark, handling over 1TB of daily data and improving overall processing speed by 30%. \\nDesigned and maintained data architectures on AWS (S3, EC2, Glue, Lambda, Redshift), achieving a 25% cost reduction through optimized resource allocation and storage. \\nIntegrated real-time data streaming with Apache Kafka and AWS Kinesis, decreasing data processing latency by 20% for time-sensitive applications. \\n\\nData Engineer / ML Engineer, Bright Horizons, USA \\n• \\n\\nAutomated data validation and monitoring systems using Python, SQL scripts, and ML-based anomaly detection techniques. \\nDeveloped and optimized data lake and ETL pipelines on AWS using Glue, EMR, and Lambda, handling 2TB+ data daily. \\n\\nDeveloped a K-Means clustering model for e-commerce data segmentation, enabling real-time analytics and deriving insights. \\n\\nDeveloped an XGBoost-based anomaly detection system - This improved data flow tracking and reduced audit errors by 90%. \\n\\nJun 2023 – Jul 2024 \\n\\n•  Managed ETL operations with Informatica, integrating churn prediction using Logistic Regression that reduced churn by 15%. \\n• \\n\\nImproved MySQL, PostgreSQL, and HBase databases, reducing query execution times by 35% and enhancing data retrieval efficiency for analytics. \\n\\n• \\n\\n• \\n• \\n\\nSupported data warehousing efforts using Snowflake and Google BigQuery, leading to a 20% improvement in data accessibility and reporting speed for business stakeholder. \\nImplemented best practices for data modeling, normalization, and warehousing, ensuring clean, structured, and high-quality datasets, improving data integrity by 10%. \\nAdhered to Agile and Kanban methodologies, delivering features on time with 100% sprint success rate. \\n\\nData Engineer, Tata Consultancy Services, India \\n• \\n\\nJan 2020 – Aug 2022 \\nEngineered robust data pipelines utilizing Apache Airflow and PySpark, achieving a 30% increase in speed: enhanced workflow efficiency by reducing manual intervention. \\nBuilt scalable batch and streaming data pipelines using Apache Spark, Airflow, and Hive for an international banking client. \\nLed the adoption of CI/CD pipelines for data pipelines using GitHub Actions and Terraform. \\nEnhanced data retrieval and storage by designing efficient SQL queries and indexing strategies in MySQL and PostgreSQL, resulting in a 30% boost in query performance. \\nIntegrated data from 10+ sources (SQL, APIs, JSON, flat files) into Azure Data Lake, enabling centralized analytics. \\nEmployed Docker and Kubernetes for containerizing and orchestrating data applications, reducing deployment times by 20% and improving scalability. \\nCreated  real-time  data  dashboards  using  Power  BI  and  Tableau,  increasing  insight  delivery  speed  by  25%  and  improving  reporting  accuracy  for stakeholders. \\nAssisted in implementing Azure Synapse and Databricks solutions, enhancing data analytics capabilities and improving query performance by 20%. \\n\\nContributed to data validation and governance efforts, reducing data discrepancies by 10% and improving overall data quality.',\n",
       " 'education': 'Masters in Data Analytics Engineering \\nNortheastern University and Boston, MA, USA (GPA: 4.0) \\n\\nBachelors in Electronics and Communication Engineering  \\nVellore Institute of Technology (VIT) and Vellore, Tamil Nadu, India (GPA: 3.7)   \\n\\nSept 2022 – Aug 2024 \\n\\nJuly 2017 – June 2021 \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n•'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d508da5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lambda',\n",
       " 'R.',\n",
       " 'EMR)',\n",
       " 'SQL\\u202fServer',\n",
       " 'EDA',\n",
       " 'Apache\\u202fFlink',\n",
       " 'NumPy',\n",
       " 'CI/CD.',\n",
       " 'ML Flow',\n",
       " 'seaborn',\n",
       " 'Informatica',\n",
       " 'MySQL',\n",
       " 'DASH',\n",
       " 'ChromaDB',\n",
       " 'Python Libraries: NLTK',\n",
       " 'TIDAL.',\n",
       " 'Data Processing & Orchestration: Apache\\u202fSpark',\n",
       " 'PySpark',\n",
       " 'statsmodels',\n",
       " 'Statistical Modeling',\n",
       " 'Cloud & Warehousing: AWS\\u202f(S3',\n",
       " 'Trend Analysis',\n",
       " 'Dask',\n",
       " 'Kanban',\n",
       " 'matplotlib',\n",
       " 'TensorFlow',\n",
       " 'GitHub\\u202fActions',\n",
       " 'Visualization & BI: Power BI',\n",
       " 'RAG pipelines',\n",
       " 'VertexAI)',\n",
       " 'Talend',\n",
       " 'GCP\\u202f(BigQuery',\n",
       " 'Hadoop',\n",
       " 'dbt',\n",
       " 'Databases: Oracle',\n",
       " 'Cosmos\\u202fDB',\n",
       " 'Kubernetes',\n",
       " 'Shell',\n",
       " 'Databricks',\n",
       " 'Kinesis',\n",
       " 'Teradata',\n",
       " 'Airflow',\n",
       " 'Azure\\u202f(Synapse',\n",
       " 'Supervised & Unsupervised Learning',\n",
       " 'Plotly',\n",
       " 'TFDV',\n",
       " 'SQL',\n",
       " 'Snowflake.',\n",
       " 'Feature Engineering',\n",
       " 'Scikit-Learn',\n",
       " 'Terraform',\n",
       " 'GenAI & Machine Learning: OpenAI embeddings',\n",
       " 'pydantic',\n",
       " 'Glue',\n",
       " 'Data\\u202fFactory)',\n",
       " 'Programming & Scripting: Python',\n",
       " 'AWS\\u202fKinesis.',\n",
       " 'Streaming & Real‑Time Analytics: Apache\\u202fKafka',\n",
       " 'ADLS',\n",
       " '•  Methodologies & Governance: Agile‑Scrum',\n",
       " 'Pandas',\n",
       " 'Redshift',\n",
       " 'Data\\u202fWarehousing',\n",
       " 'PyTorch',\n",
       " 'Containerization & DevOps: Docker',\n",
       " 'GDPR/HIPAA compliance.',\n",
       " 'Data\\u202fModelling',\n",
       " 'MongoDB',\n",
       " 'Model Evaluation metrics.',\n",
       " 'SSIS',\n",
       " 'Tableau',\n",
       " 'PostgreSQL',\n",
       " 'NoSQL.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(sections['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17522174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_resume_parse(resume_path):\n",
    "    text = extract_text_from_file(resume_path)\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    name = extract_name(text)\n",
    "    email = extract_email(text)\n",
    "    phone = extract_phone(text)\n",
    "    skills = extract_skills(sections.get('skills', '')) if 'skills' in sections else []\n",
    "    education = extract_education(sections.get('education', '')) if 'education' in sections else []\n",
    "    certifications = extract_certifications(sections.get('certifications', '')) if 'certifications' in sections else []\n",
    "    experience = extract_experience(sections.get('experience', '')) if 'experience' in sections else []\n",
    "    projects = []  # Implement if needed: extract_projects(sections.get('projects', ''))\n",
    "    soft_skills = []  # Optional: can do keyword match or use spaCy's NER\n",
    "    other_notes = []\n",
    "\n",
    "    out = {\n",
    "        'name': name,\n",
    "        'email': email,\n",
    "        'phone': phone,\n",
    "        'skills': skills,\n",
    "        'education': education,\n",
    "        'certifications': certifications,\n",
    "        'past_roles': experience,\n",
    "        'projects': projects,\n",
    "        'soft_skills': soft_skills,\n",
    "        'other_notes': other_notes,\n",
    "        'total_years_of_experience': calc_total_exp(experience)\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7695fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "def pdf_to_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "text = pdf_to_text(r\"C:\\Users\\hrith\\Projects\\Screening_Agent\\data\\resumes\\Hrithik_Resume.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab4a6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HRITHIK SARDA  \\nData Scientist \\nBoston MA, USA | +1 (978)-654-0445 | Email: Hrithik.sarda1@gmail.com | LinkedIn \\n\\nProfessional Summary \\nResults-driven Data Scientist with 4+ years of experience in designing, building, and optimizing data pipelines, ETL processes, and scalable architectures across Data, ML, and \\nAI domains. Proficient in Python, SQL, Apache Spark, AWS, and cloud-native tools, with hands-on exposure to machine learning workflows and AI integration. Skilled in data \\nmodeling, data warehousing, real-time data processing, and deploying end-to-end ML solutions. Adept at collaborating with cross-functional teams to transform complex business \\nrequirements  into  actionable  data-driven  insights.  Passionate  about  leveraging  GenAI  &  advanced  analytics  to  empower  organizational  decision-making  and  innovation. \\n\\nSkills \\n• \\n• \\n• \\n• \\n• \\n\\nProgramming & Scripting: Python, SQL,\\u202fPySpark, Shell, R. \\n\\nPython Libraries: NLTK, TensorFlow, Pandas, Scikit-Learn, NumPy, TFDV, PyTorch, Airflow, ML Flow, statsmodels, Dask, pydantic, DASH \\n\\nCloud & Warehousing: AWS\\u202f(S3, Glue, Redshift, Lambda,\\u202fKinesis, EMR), Azure\\u202f(Synapse, ADLS,\\u202fDatabricks, Data\\u202fFactory), GCP\\u202f(BigQuery, VertexAI), Snowflake. \\n\\nData Processing & Orchestration: Apache\\u202fSpark, Hadoop, dbt, Airflow, Talend, Informatica, SSIS, TIDAL. \\n\\nDatabases: Oracle, SQL\\u202fServer, PostgreSQL, MySQL, Teradata, MongoDB, Cosmos\\u202fDB, NoSQL. \\n\\n• \\n\\nStreaming & Real‑Time Analytics: Apache\\u202fKafka, Apache\\u202fFlink, AWS\\u202fKinesis. \\n\\nContainerization & DevOps: Docker, Kubernetes, Terraform, GitHub\\u202fActions, CI/CD. \\n\\nVisualization & BI: Power BI, Tableau, EDA, Statistical Modeling, Trend Analysis, matplotlib, seaborn, Plotly \\n\\n• \\n• \\n• \\n•  Methodologies & Governance: Agile‑Scrum, Kanban, Data\\u202fModelling, Data\\u202fWarehousing, GDPR/HIPAA compliance. \\n\\nGenAI & Machine Learning: OpenAI embeddings, ChromaDB, RAG pipelines, Supervised & Unsupervised Learning, Feature Engineering, Model Evaluation metrics. \\n\\nProfessional Experience \\nSenior Data Scientist, Pfizer, USA \\n• \\n\\nAug 2024 – Present \\nDeveloped a RAG pipeline ingesting PDFs, chunking text, embedding with OpenAI text-embedding-3-small, and storing vectors in ChromaDB for <100ms semantic search. \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\nIntegrated GenAI-powered pipelines to automate feature extraction, data augmentation, and metadata management for AI model deployment. \\nAutomated ETL processes using Apache Airflow and Talend, reducing manual intervention by 40% and improving data flow reliability. \\nProductionised a fully-automated sepsis-prediction pipeline on GCP (BigQuery, Airflow, Kubernetes); orchestrated MLflow model training and Vertex\\u202fAI hyper-tuning, \\nachieving 93% accuracy\\u202f/\\u202f0.95\\u202fF1 and continuous retraining via Cloud\\u202fComposer. \\nTrained a transfer-learning Inception-V3 model on 33\\u202fk dermoscopic images; leveraged GAN-based augmentation to correct class imbalance (1:2 ratio) and delivered 98.8% \\naccuracy with 0.95 precision–recall, supporting faster clinical diagnosis. \\nBuilt and optimized data pipelines in Python, SQL, and PySpark with Apache Spark, handling over 1TB of daily data and improving overall processing speed by 30%. \\nDesigned and maintained data architectures on AWS (S3, EC2, Glue, Lambda, Redshift), achieving a 25% cost reduction through optimized resource allocation and storage. \\nIntegrated real-time data streaming with Apache Kafka and AWS Kinesis, decreasing data processing latency by 20% for time-sensitive applications. \\n\\nData Engineer / ML Engineer, Bright Horizons, USA \\n• \\n\\nAutomated data validation and monitoring systems using Python, SQL scripts, and ML-based anomaly detection techniques. \\nDeveloped and optimized data lake and ETL pipelines on AWS using Glue, EMR, and Lambda, handling 2TB+ data daily. \\n\\nDeveloped a K-Means clustering model for e-commerce data segmentation, enabling real-time analytics and deriving insights. \\n\\nDeveloped an XGBoost-based anomaly detection system - This improved data flow tracking and reduced audit errors by 90%. \\n\\nJun 2023 – Jul 2024 \\n\\n•  Managed ETL operations with Informatica, integrating churn prediction using Logistic Regression that reduced churn by 15%. \\n• \\n\\nImproved MySQL, PostgreSQL, and HBase databases, reducing query execution times by 35% and enhancing data retrieval efficiency for analytics. \\n\\n• \\n\\n• \\n• \\n\\nSupported data warehousing efforts using Snowflake and Google BigQuery, leading to a 20% improvement in data accessibility and reporting speed for business stakeholder. \\nImplemented best practices for data modeling, normalization, and warehousing, ensuring clean, structured, and high-quality datasets, improving data integrity by 10%. \\nAdhered to Agile and Kanban methodologies, delivering features on time with 100% sprint success rate. \\n\\nData Engineer, Tata Consultancy Services, India \\n• \\n\\nJan 2020 – Aug 2022 \\nEngineered robust data pipelines utilizing Apache Airflow and PySpark, achieving a 30% increase in speed: enhanced workflow efficiency by reducing manual intervention. \\nBuilt scalable batch and streaming data pipelines using Apache Spark, Airflow, and Hive for an international banking client. \\nLed the adoption of CI/CD pipelines for data pipelines using GitHub Actions and Terraform. \\nEnhanced data retrieval and storage by designing efficient SQL queries and indexing strategies in MySQL and PostgreSQL, resulting in a 30% boost in query performance. \\nIntegrated data from 10+ sources (SQL, APIs, JSON, flat files) into Azure Data Lake, enabling centralized analytics. \\nEmployed Docker and Kubernetes for containerizing and orchestrating data applications, reducing deployment times by 20% and improving scalability. \\nCreated  real-time  data  dashboards  using  Power  BI  and  Tableau,  increasing  insight  delivery  speed  by  25%  and  improving  reporting  accuracy  for stakeholders. \\nAssisted in implementing Azure Synapse and Databricks solutions, enhancing data analytics capabilities and improving query performance by 20%. \\n\\nContributed to data validation and governance efforts, reducing data discrepancies by 10% and improving overall data quality. \\n\\nEducation \\nMasters in Data Analytics Engineering \\nNortheastern University and Boston, MA, USA (GPA: 4.0) \\n\\nBachelors in Electronics and Communication Engineering  \\nVellore Institute of Technology (VIT) and Vellore, Tamil Nadu, India (GPA: 3.7)   \\n\\nSept 2022 – Aug 2024 \\n\\nJuly 2017 – June 2021 \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6cb157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRITHIK SARDA\n",
      "Data Scientist\n",
      "Boston MA, USA | +1 (978)-654-0445 | Email: Hrithik.sarda1@gmail.com | LinkedIn\n",
      "\n",
      "Professional Summary\n",
      "Results-driven Data Scientist with 4+ years of experience in designing, building, and optimizing data pipelines, ETL processes, and scalable architectures across Data, ML, and\n",
      "AI domains. Proficient in Python, SQL, Apache Spark, AWS, and cloud-native tools, with hands-on exposure to machine learning workflows and AI integration. Skilled in data\n",
      "modeling, data warehousing, real-time data processing, and deploying end-to-end ML solutions. Adept at collaborating with cross-functional teams to transform complex business\n",
      "requirements  into  actionable  data-driven  insights.  Passionate  about  leveraging  GenAI  &  advanced  analytics  to  empower  organizational  decision-making  and  innovation.\n",
      "\n",
      "Skills\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "\n",
      "Programming & Scripting: Python, SQL, PySpark, Shell, R.\n",
      "\n",
      "Python Libraries: NLTK, TensorFlow, Pandas, Scikit-Learn, NumPy, TFDV, PyTorch, Airflow, ML Flow, statsmodels, Dask, pydantic, DASH\n",
      "\n",
      "Cloud & Warehousing: AWS (S3, Glue, Redshift, Lambda, Kinesis, EMR), Azure (Synapse, ADLS, Databricks, Data Factory), GCP (BigQuery, VertexAI), Snowflake.\n",
      "\n",
      "Data Processing & Orchestration: Apache Spark, Hadoop, dbt, Airflow, Talend, Informatica, SSIS, TIDAL.\n",
      "\n",
      "Databases: Oracle, SQL Server, PostgreSQL, MySQL, Teradata, MongoDB, Cosmos DB, NoSQL.\n",
      "\n",
      "•\n",
      "\n",
      "Streaming & Real‑Time Analytics: Apache Kafka, Apache Flink, AWS Kinesis.\n",
      "\n",
      "Containerization & DevOps: Docker, Kubernetes, Terraform, GitHub Actions, CI/CD.\n",
      "\n",
      "Visualization & BI: Power BI, Tableau, EDA, Statistical Modeling, Trend Analysis, matplotlib, seaborn, Plotly\n",
      "\n",
      "•\n",
      "•\n",
      "•\n",
      "•  Methodologies & Governance: Agile‑Scrum, Kanban, Data Modelling, Data Warehousing, GDPR/HIPAA compliance.\n",
      "\n",
      "GenAI & Machine Learning: OpenAI embeddings, ChromaDB, RAG pipelines, Supervised & Unsupervised Learning, Feature Engineering, Model Evaluation metrics.\n",
      "\n",
      "Professional Experience\n",
      "Senior Data Scientist, Pfizer, USA\n",
      "•\n",
      "\n",
      "Aug 2024 – Present\n",
      "Developed a RAG pipeline ingesting PDFs, chunking text, embedding with OpenAI text-embedding-3-small, and storing vectors in ChromaDB for <100ms semantic search.\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "Integrated GenAI-powered pipelines to automate feature extraction, data augmentation, and metadata management for AI model deployment.\n",
      "Automated ETL processes using Apache Airflow and Talend, reducing manual intervention by 40% and improving data flow reliability.\n",
      "Productionised a fully-automated sepsis-prediction pipeline on GCP (BigQuery, Airflow, Kubernetes); orchestrated MLflow model training and Vertex AI hyper-tuning,\n",
      "achieving 93% accuracy / 0.95 F1 and continuous retraining via Cloud Composer.\n",
      "Trained a transfer-learning Inception-V3 model on 33 k dermoscopic images; leveraged GAN-based augmentation to correct class imbalance (1:2 ratio) and delivered 98.8%\n",
      "accuracy with 0.95 precision–recall, supporting faster clinical diagnosis.\n",
      "Built and optimized data pipelines in Python, SQL, and PySpark with Apache Spark, handling over 1TB of daily data and improving overall processing speed by 30%.\n",
      "Designed and maintained data architectures on AWS (S3, EC2, Glue, Lambda, Redshift), achieving a 25% cost reduction through optimized resource allocation and storage.\n",
      "Integrated real-time data streaming with Apache Kafka and AWS Kinesis, decreasing data processing latency by 20% for time-sensitive applications.\n",
      "\n",
      "Data Engineer / ML Engineer, Bright Horizons, USA\n",
      "•\n",
      "\n",
      "Automated data validation and monitoring systems using Python, SQL scripts, and ML-based anomaly detection techniques.\n",
      "Developed and optimized data lake and ETL pipelines on AWS using Glue, EMR, and Lambda, handling 2TB+ data daily.\n",
      "\n",
      "Developed a K-Means clustering model for e-commerce data segmentation, enabling real-time analytics and deriving insights.\n",
      "\n",
      "Developed an XGBoost-based anomaly detection system - This improved data flow tracking and reduced audit errors by 90%.\n",
      "\n",
      "Jun 2023 – Jul 2024\n",
      "\n",
      "•  Managed ETL operations with Informatica, integrating churn prediction using Logistic Regression that reduced churn by 15%.\n",
      "•\n",
      "\n",
      "Improved MySQL, PostgreSQL, and HBase databases, reducing query execution times by 35% and enhancing data retrieval efficiency for analytics.\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "•\n",
      "\n",
      "Supported data warehousing efforts using Snowflake and Google BigQuery, leading to a 20% improvement in data accessibility and reporting speed for business stakeholder.\n",
      "Implemented best practices for data modeling, normalization, and warehousing, ensuring clean, structured, and high-quality datasets, improving data integrity by 10%.\n",
      "Adhered to Agile and Kanban methodologies, delivering features on time with 100% sprint success rate.\n",
      "\n",
      "Data Engineer, Tata Consultancy Services, India\n",
      "•\n",
      "\n",
      "Jan 2020 – Aug 2022\n",
      "Engineered robust data pipelines utilizing Apache Airflow and PySpark, achieving a 30% increase in speed: enhanced workflow efficiency by reducing manual intervention.\n",
      "Built scalable batch and streaming data pipelines using Apache Spark, Airflow, and Hive for an international banking client.\n",
      "Led the adoption of CI/CD pipelines for data pipelines using GitHub Actions and Terraform.\n",
      "Enhanced data retrieval and storage by designing efficient SQL queries and indexing strategies in MySQL and PostgreSQL, resulting in a 30% boost in query performance.\n",
      "Integrated data from 10+ sources (SQL, APIs, JSON, flat files) into Azure Data Lake, enabling centralized analytics.\n",
      "Employed Docker and Kubernetes for containerizing and orchestrating data applications, reducing deployment times by 20% and improving scalability.\n",
      "Created  real-time  data  dashboards  using  Power  BI  and  Tableau,  increasing  insight  delivery  speed  by  25%  and  improving  reporting  accuracy  for stakeholders.\n",
      "Assisted in implementing Azure Synapse and Databricks solutions, enhancing data analytics capabilities and improving query performance by 20%.\n",
      "\n",
      "Contributed to data validation and governance efforts, reducing data discrepancies by 10% and improving overall data quality.\n",
      "\n",
      "Education\n",
      "Masters in Data Analytics Engineering\n",
      "Northeastern University and Boston, MA, USA (GPA: 4.0)\n",
      "\n",
      "Bachelors in Electronics and Communication Engineering\n",
      "Vellore Institute of Technology (VIT) and Vellore, Tamil Nadu, India (GPA: 3.7)\n",
      "\n",
      "Sept 2022 – Aug 2024\n",
      "\n",
      "July 2017 – June 2021\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in text.split('\\n'):\n",
    "    print(line.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
