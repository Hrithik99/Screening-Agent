{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b43c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import spacy\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import Document\n",
    "from dateutil import parser as dateparser\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialize spaCy once\n",
    "NLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2668c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should expand these lists for production!\n",
    "SKILLS_LIST = [\n",
    "    'Python', 'SQL', 'PySpark', 'Shell', 'R', 'NLTK', 'TensorFlow', 'Pandas', 'Scikit-Learn', 'NumPy',\n",
    "    'TFDV', 'PyTorch', 'Airflow', 'ML Flow', 'statsmodels', 'Dask', 'pydantic', 'DASH', 'AWS',\n",
    "    'Azure', 'GCP', 'Snowflake', 'Apache Spark', 'Hadoop', 'dbt', 'Talend', 'Informatica', 'SSIS',\n",
    "    'TIDAL', 'Oracle', 'SQL Server', 'PostgreSQL', 'MySQL', 'Teradata', 'MongoDB', 'Cosmos DB',\n",
    "    'NoSQL', 'Apache Kafka', 'Apache Flink', 'Docker', 'Kubernetes', 'Terraform', 'GitHub Actions',\n",
    "    'CI/CD', 'Power BI', 'Tableau', 'EDA', 'Statistical Modeling', 'Trend Analysis', 'matplotlib',\n",
    "    'seaborn', 'Plotly', 'Agile-Scrum', 'Kanban', 'Data Modelling', 'Data Warehousing', 'GDPR/HIPAA compliance',\n",
    "    'OpenAI embeddings', 'ChromaDB', 'RAG pipelines', 'Supervised & Unsupervised Learning', 'Feature Engineering', 'Model Evaluation metrics'\n",
    "]\n",
    "DEGREE_KEYWORDS = [\n",
    "    'bachelor', 'master', 'doctor', 'phd', 'msc', 'bachelors', 'masters', 'engineering', 'm.tech', 'b.tech'\n",
    "]\n",
    "CERT_KEYWORDS = ['certification', 'certificate', 'certified', 'certifications', 'licenses']\n",
    "SECTION_HEADERS = {\n",
    "    'education': ['education', 'academic background', 'academics'],\n",
    "    'experience': ['professional experience', 'work experience', 'employment', 'experience'],\n",
    "    'skills': ['skills', 'technical skills', 'key skills'],\n",
    "    'certifications': ['certifications', 'certificates', 'licenses'],\n",
    "    'projects': ['projects', 'key projects', 'personal projects']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eb9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def docx_to_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_file(path):\n",
    "    if path.endswith('.pdf'):\n",
    "        return pdf_to_text(path)\n",
    "    elif path.endswith('.docx'):\n",
    "        return docx_to_text(path)\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5a87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    lines = text.split('\\n')\n",
    "    section_map = {}\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    def header_key(line):\n",
    "        line_clean = line.strip().lower()\n",
    "        for key, variants in SECTION_HEADERS.items():\n",
    "            if any(line_clean.startswith(h) for h in variants):\n",
    "                return key\n",
    "        return None\n",
    "\n",
    "    for line in lines:\n",
    "        section = header_key(line)\n",
    "        if section:\n",
    "            if current_section and buffer:\n",
    "                section_map[current_section] = '\\n'.join(buffer).strip()\n",
    "                buffer = []\n",
    "            current_section = section\n",
    "        elif current_section:\n",
    "            buffer.append(line)\n",
    "    # Capture last section\n",
    "    if current_section and buffer:\n",
    "        section_map[current_section] = '\\n'.join(buffer).strip()\n",
    "    return section_map\n",
    "\n",
    "def extract_skills(skills_text):\n",
    "    skills_found = set()\n",
    "    text_lower = skills_text.lower()\n",
    "    '''\n",
    "    for skill in SKILLS_LIST:\n",
    "        if re.search(r'\\b' + re.escape(skill.lower()) + r'\\b', text_lower):\n",
    "            skills_found.add(skill)\n",
    "    '''\n",
    "    # Add anything in a comma/list line in skills section\n",
    "    for line in skills_text.split('\\n'):\n",
    "        if ',' in line:\n",
    "            for word in line.split(','):\n",
    "                word_clean = word.strip()\n",
    "                if word_clean and word_clean not in skills_found:\n",
    "                    skills_found.add(word_clean)\n",
    "    return list(skills_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063e3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=extract_text_from_file(r\"C:\\Users\\hrith\\Projects\\Screening_Agent\\data\\resumes\\Hrithik_Resume.pdf\")\n",
    "sections = extract_sections(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46935837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': '• \\n• \\n• \\n• \\n• \\n\\nProgramming & Scripting: Python, SQL,\\u202fPySpark, Shell, R. \\n\\nPython Libraries: NLTK, TensorFlow, Pandas, Scikit-Learn, NumPy, TFDV, PyTorch, Airflow, ML Flow, statsmodels, Dask, pydantic, DASH \\n\\nCloud & Warehousing: AWS\\u202f(S3, Glue, Redshift, Lambda,\\u202fKinesis, EMR), Azure\\u202f(Synapse, ADLS,\\u202fDatabricks, Data\\u202fFactory), GCP\\u202f(BigQuery, VertexAI), Snowflake. \\n\\nData Processing & Orchestration: Apache\\u202fSpark, Hadoop, dbt, Airflow, Talend, Informatica, SSIS, TIDAL. \\n\\nDatabases: Oracle, SQL\\u202fServer, PostgreSQL, MySQL, Teradata, MongoDB, Cosmos\\u202fDB, NoSQL. \\n\\n• \\n\\nStreaming & Real‑Time Analytics: Apache\\u202fKafka, Apache\\u202fFlink, AWS\\u202fKinesis. \\n\\nContainerization & DevOps: Docker, Kubernetes, Terraform, GitHub\\u202fActions, CI/CD. \\n\\nVisualization & BI: Power BI, Tableau, EDA, Statistical Modeling, Trend Analysis, matplotlib, seaborn, Plotly \\n\\n• \\n• \\n• \\n•  Methodologies & Governance: Agile‑Scrum, Kanban, Data\\u202fModelling, Data\\u202fWarehousing, GDPR/HIPAA compliance. \\n\\nGenAI & Machine Learning: OpenAI embeddings, ChromaDB, RAG pipelines, Supervised & Unsupervised Learning, Feature Engineering, Model Evaluation metrics.',\n",
       " 'experience': 'Senior Data Scientist, Pfizer, USA \\n• \\n\\nAug 2024 – Present \\nDeveloped a RAG pipeline ingesting PDFs, chunking text, embedding with OpenAI text-embedding-3-small, and storing vectors in ChromaDB for <100ms semantic search. \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\nIntegrated GenAI-powered pipelines to automate feature extraction, data augmentation, and metadata management for AI model deployment. \\nAutomated ETL processes using Apache Airflow and Talend, reducing manual intervention by 40% and improving data flow reliability. \\nProductionised a fully-automated sepsis-prediction pipeline on GCP (BigQuery, Airflow, Kubernetes); orchestrated MLflow model training and Vertex\\u202fAI hyper-tuning, \\nachieving 93% accuracy\\u202f/\\u202f0.95\\u202fF1 and continuous retraining via Cloud\\u202fComposer. \\nTrained a transfer-learning Inception-V3 model on 33\\u202fk dermoscopic images; leveraged GAN-based augmentation to correct class imbalance (1:2 ratio) and delivered 98.8% \\naccuracy with 0.95 precision–recall, supporting faster clinical diagnosis. \\nBuilt and optimized data pipelines in Python, SQL, and PySpark with Apache Spark, handling over 1TB of daily data and improving overall processing speed by 30%. \\nDesigned and maintained data architectures on AWS (S3, EC2, Glue, Lambda, Redshift), achieving a 25% cost reduction through optimized resource allocation and storage. \\nIntegrated real-time data streaming with Apache Kafka and AWS Kinesis, decreasing data processing latency by 20% for time-sensitive applications. \\n\\nData Engineer / ML Engineer, Bright Horizons, USA \\n• \\n\\nAutomated data validation and monitoring systems using Python, SQL scripts, and ML-based anomaly detection techniques. \\nDeveloped and optimized data lake and ETL pipelines on AWS using Glue, EMR, and Lambda, handling 2TB+ data daily. \\n\\nDeveloped a K-Means clustering model for e-commerce data segmentation, enabling real-time analytics and deriving insights. \\n\\nDeveloped an XGBoost-based anomaly detection system - This improved data flow tracking and reduced audit errors by 90%. \\n\\nJun 2023 – Jul 2024 \\n\\n•  Managed ETL operations with Informatica, integrating churn prediction using Logistic Regression that reduced churn by 15%. \\n• \\n\\nImproved MySQL, PostgreSQL, and HBase databases, reducing query execution times by 35% and enhancing data retrieval efficiency for analytics. \\n\\n• \\n\\n• \\n• \\n\\nSupported data warehousing efforts using Snowflake and Google BigQuery, leading to a 20% improvement in data accessibility and reporting speed for business stakeholder. \\nImplemented best practices for data modeling, normalization, and warehousing, ensuring clean, structured, and high-quality datasets, improving data integrity by 10%. \\nAdhered to Agile and Kanban methodologies, delivering features on time with 100% sprint success rate. \\n\\nData Engineer, Tata Consultancy Services, India \\n• \\n\\nJan 2020 – Aug 2022 \\nEngineered robust data pipelines utilizing Apache Airflow and PySpark, achieving a 30% increase in speed: enhanced workflow efficiency by reducing manual intervention. \\nBuilt scalable batch and streaming data pipelines using Apache Spark, Airflow, and Hive for an international banking client. \\nLed the adoption of CI/CD pipelines for data pipelines using GitHub Actions and Terraform. \\nEnhanced data retrieval and storage by designing efficient SQL queries and indexing strategies in MySQL and PostgreSQL, resulting in a 30% boost in query performance. \\nIntegrated data from 10+ sources (SQL, APIs, JSON, flat files) into Azure Data Lake, enabling centralized analytics. \\nEmployed Docker and Kubernetes for containerizing and orchestrating data applications, reducing deployment times by 20% and improving scalability. \\nCreated  real-time  data  dashboards  using  Power  BI  and  Tableau,  increasing  insight  delivery  speed  by  25%  and  improving  reporting  accuracy  for stakeholders. \\nAssisted in implementing Azure Synapse and Databricks solutions, enhancing data analytics capabilities and improving query performance by 20%. \\n\\nContributed to data validation and governance efforts, reducing data discrepancies by 10% and improving overall data quality.',\n",
       " 'education': 'Masters in Data Analytics Engineering \\nNortheastern University and Boston, MA, USA (GPA: 4.0) \\n\\nBachelors in Electronics and Communication Engineering  \\nVellore Institute of Technology (VIT) and Vellore, Tamil Nadu, India (GPA: 3.7)   \\n\\nSept 2022 – Aug 2024 \\n\\nJuly 2017 – June 2021 \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\n•'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d508da5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lambda',\n",
       " 'R.',\n",
       " 'EMR)',\n",
       " 'SQL\\u202fServer',\n",
       " 'EDA',\n",
       " 'Apache\\u202fFlink',\n",
       " 'NumPy',\n",
       " 'CI/CD.',\n",
       " 'ML Flow',\n",
       " 'seaborn',\n",
       " 'Informatica',\n",
       " 'MySQL',\n",
       " 'DASH',\n",
       " 'ChromaDB',\n",
       " 'Python Libraries: NLTK',\n",
       " 'TIDAL.',\n",
       " 'Data Processing & Orchestration: Apache\\u202fSpark',\n",
       " 'PySpark',\n",
       " 'statsmodels',\n",
       " 'Statistical Modeling',\n",
       " 'Cloud & Warehousing: AWS\\u202f(S3',\n",
       " 'Trend Analysis',\n",
       " 'Dask',\n",
       " 'Kanban',\n",
       " 'matplotlib',\n",
       " 'TensorFlow',\n",
       " 'GitHub\\u202fActions',\n",
       " 'Visualization & BI: Power BI',\n",
       " 'RAG pipelines',\n",
       " 'VertexAI)',\n",
       " 'Talend',\n",
       " 'GCP\\u202f(BigQuery',\n",
       " 'Hadoop',\n",
       " 'dbt',\n",
       " 'Databases: Oracle',\n",
       " 'Cosmos\\u202fDB',\n",
       " 'Kubernetes',\n",
       " 'Shell',\n",
       " 'Databricks',\n",
       " 'Kinesis',\n",
       " 'Teradata',\n",
       " 'Airflow',\n",
       " 'Azure\\u202f(Synapse',\n",
       " 'Supervised & Unsupervised Learning',\n",
       " 'Plotly',\n",
       " 'TFDV',\n",
       " 'SQL',\n",
       " 'Snowflake.',\n",
       " 'Feature Engineering',\n",
       " 'Scikit-Learn',\n",
       " 'Terraform',\n",
       " 'GenAI & Machine Learning: OpenAI embeddings',\n",
       " 'pydantic',\n",
       " 'Glue',\n",
       " 'Data\\u202fFactory)',\n",
       " 'Programming & Scripting: Python',\n",
       " 'AWS\\u202fKinesis.',\n",
       " 'Streaming & Real‑Time Analytics: Apache\\u202fKafka',\n",
       " 'ADLS',\n",
       " '•  Methodologies & Governance: Agile‑Scrum',\n",
       " 'Pandas',\n",
       " 'Redshift',\n",
       " 'Data\\u202fWarehousing',\n",
       " 'PyTorch',\n",
       " 'Containerization & DevOps: Docker',\n",
       " 'GDPR/HIPAA compliance.',\n",
       " 'Data\\u202fModelling',\n",
       " 'MongoDB',\n",
       " 'Model Evaluation metrics.',\n",
       " 'SSIS',\n",
       " 'Tableau',\n",
       " 'PostgreSQL',\n",
       " 'NoSQL.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(sections['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17522174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_resume_parse(resume_path):\n",
    "    text = extract_text_from_file(resume_path)\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    name = extract_name(text)\n",
    "    email = extract_email(text)\n",
    "    phone = extract_phone(text)\n",
    "    skills = extract_skills(sections.get('skills', '')) if 'skills' in sections else []\n",
    "    education = extract_education(sections.get('education', '')) if 'education' in sections else []\n",
    "    certifications = extract_certifications(sections.get('certifications', '')) if 'certifications' in sections else []\n",
    "    experience = extract_experience(sections.get('experience', '')) if 'experience' in sections else []\n",
    "    projects = []  # Implement if needed: extract_projects(sections.get('projects', ''))\n",
    "    soft_skills = []  # Optional: can do keyword match or use spaCy's NER\n",
    "    other_notes = []\n",
    "\n",
    "    out = {\n",
    "        'name': name,\n",
    "        'email': email,\n",
    "        'phone': phone,\n",
    "        'skills': skills,\n",
    "        'education': education,\n",
    "        'certifications': certifications,\n",
    "        'past_roles': experience,\n",
    "        'projects': projects,\n",
    "        'soft_skills': soft_skills,\n",
    "        'other_notes': other_notes,\n",
    "        'total_years_of_experience': calc_total_exp(experience)\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695fd62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/resumes/Hrithik_Resume.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpdf_to_text\u001b[39m(pdf_path):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_text(pdf_path)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m text = \u001b[43mpdf_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/resumes/Hrithik_Resume.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mpdf_to_text\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpdf_to_text\u001b[39m(pdf_path):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hrith\\Projects\\Screening_Agent\\.screening\\Lib\\site-packages\\pdfminer\\high_level.py:171\u001b[39m, in \u001b[36mextract_text\u001b[39m\u001b[34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m laparams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    169\u001b[39m     laparams = LAParams()\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mopen_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp, StringIO() \u001b[38;5;28;01mas\u001b[39;00m output_string:\n\u001b[32m    172\u001b[39m     fp = cast(BinaryIO, fp)  \u001b[38;5;66;03m# we opened in binary mode\u001b[39;00m\n\u001b[32m    173\u001b[39m     rsrcmgr = PDFResourceManager(caching=caching)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hrith\\Projects\\Screening_Agent\\.screening\\Lib\\site-packages\\pdfminer\\utils.py:51\u001b[39m, in \u001b[36mopen_filename.__init__\u001b[39m\u001b[34m(self, filename, *args, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m     filename = \u001b[38;5;28mstr\u001b[39m(filename)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28mself\u001b[39m.file_handler: AnyIO = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.closing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, io.IOBase):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/resumes/Hrithik_Resume.pdf'"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "def pdf_to_text(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "text = pdf_to_text(\"C:\\Users\\hrith\\Projects\\Screening_Agent\\data\\\\resumes\\Hrithik_Resume.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iine in text.split('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".screening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
